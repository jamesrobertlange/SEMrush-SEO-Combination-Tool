This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-12-09T21:26:35.795Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
.gitignore
app.py
README.md
requirements.txt

================================================================
Repository Files
================================================================

================
File: .gitignore
================
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
ENV/
env/
.env
.venv

# IDE
.idea/
.vscode/
*.swp
*.swo
.project
.pydevproject
.settings

# Streamlit
.streamlit/

# Operating System
.DS_Store
Thumbs.db
*.log

# Local development
*.csv
*.zip
temp/
uploads/
downloads/
.ipynb_checkpoints

# Environment variables
.env
.env.local

# Misc
*.bak
*.tmp
*.temp

================
File: app.py
================
import streamlit as st
import pandas as pd
import numpy as np
from urllib.parse import urlparse
import json
import io
import zipfile
import base64

# Set page config first
st.set_page_config(page_title="SEMrush SEO Combo Tool by Jimmy Lange", layout="wide")

# Updated CSS with more specific selectors
st.markdown("""
<style>
    /* Style upload text like Settings */
    [data-testid="stFileUploadDropzone"] label {
        font-size: 1.25rem !important;
        font-weight: 600 !important;
        color: rgb(250, 250, 250) !important;
    }
    
    /* Button Base Styles - More specific targeting */
    .stDownloadButton button, 
    .stButton button {
        background-color: rgb(38, 39, 48) !important;
        border: 1px solid rgb(70, 72, 82) !important;
        border-radius: 4px !important;
        padding: 0.75rem 1rem !important;
        width: 100% !important;
        color: rgb(250, 250, 250) !important;
    }
    
    /* Main Download Button */
    [data-testid="stDownloadButton"]:first-of-type button {
        min-height: 100px !important;
        font-size: 1.2rem !important;
        max-width: 800px !important;
        margin: 2rem auto !important;
        display: block !important;
    }
    
    /* Secondary Download Buttons */
    div.row-widget.stHorizontal [data-testid="stDownloadButton"] button {
        min-height: 75px !important;
        font-size: 1.1rem !important;
        width: 100% !important;
    }
    
    /* Fix button container widths */
    div.row-widget.stHorizontal > div {
        flex: 1;
    }
    
    /* Center containers */
    div.block-container {
        padding-top: 2rem;
        max-width: 1200px;
        margin: 0 auto;
    }
    
    /* Download All Results ZIP styling */
    div.zip-container {
        background-color: rgb(38, 39, 48);
        border: 1px solid rgb(70, 72, 82);
        padding: 1.5rem;
        border-radius: 4px;
        text-align: center;
        max-width: 800px;
        margin: 2rem auto;
    }
    
    .zip-container a {
        color: rgb(76, 175, 80) !important;
        text-decoration: none;
        font-weight: 500;
        font-size: 1.2rem;
        display: block;
    }
    
    /* Additional Downloads text */
    .divider {
        text-align: center;
        color: rgba(250, 250, 250, 0.6);
        margin: 2rem 0;
    }
</style>
""", unsafe_allow_html=True)

def get_segment(url):
    parsed_url = urlparse(str(url))
    path = parsed_url.path.strip('/')
    segments = path.split('/')
    
    if not segments or (len(segments) == 1 and not segments[0]):
        return 'home'
    elif '.' in segments[-1]:
        return segments[-1].split('.')[0]
    else:
        return segments[-1]

def process_csv_files(uploaded_files, max_position, branded_terms, include_segments=False):
    # Read and combine CSV files
    dfs = [pd.read_csv(file) for file in uploaded_files]
    combined_df = pd.concat(dfs, ignore_index=True)
    combined_df.drop_duplicates(inplace=True)
    combined_df.reset_index(drop=True, inplace=True)

    # Filter by position
    top_pages_sem = combined_df[combined_df["Position"] <= max_position]
    
    # Select and rename columns to lowercase with underscores
    column_mapping = {
        "Keyword": "keyword",
        "Position": "position",
        "Search Volume": "search_volume",
        "Keyword Intents": "keyword_intents",
        "URL": "url",
        "Traffic": "traffic",
        "Timestamp": "timestamp"
    }
    
    top_pages_sem = top_pages_sem[list(column_mapping.keys())].rename(columns=column_mapping)
    
    # Clean and process Traffic column and convert to integer
    top_pages_sem.loc[:, 'traffic'] = top_pages_sem['traffic'].replace(',', '', regex=True).astype(int)
    top_pages_sem = top_pages_sem.sort_values(by='traffic', ascending=False)
    
    # Process timestamps
    top_pages_sem['timestamp'] = pd.to_datetime(top_pages_sem['timestamp'], errors='coerce').dt.strftime('%Y-%m-%d')
    top_pages_sem['month'] = pd.to_datetime(top_pages_sem['timestamp'], errors='coerce').dt.strftime('%Y-%m')
    top_pages_sem['date'] = top_pages_sem['month'].astype(str) + "-11"
    top_pages_sem = top_pages_sem.drop(['month', 'timestamp'], axis=1)

    # Process branded keywords if provided
    if branded_terms:
        def brandedKWS(series):
            pattern = '|'.join(r'\b{}\b'.format(term.strip()) for term in branded_terms)
            return series.str.lower().str.contains(pattern, na=False, regex=True)
        top_pages_sem["branded"] = brandedKWS(top_pages_sem["keyword"])

    # Create a copy for segment analysis
    analysis_df = top_pages_sem.copy()
    analysis_df['segment'] = analysis_df['url'].apply(get_segment)
    
    # Add segments to main output if requested
    if include_segments:
        top_pages_sem['segment'] = analysis_df['segment']
    
    # Calculate segment occurrences
    segment_occurrences = analysis_df['segment'].value_counts()

    # Analyze segments
    def agg_keywords_and_urls(group):
        sorted_group = group.sort_values('traffic', ascending=False)
        keywords = sorted_group['keyword'].tolist()[:3]
        urls = sorted_group['url'].tolist()[:3]
        traffic_sum = group['traffic'].sum()
        occurrences = segment_occurrences.get(group.name, 0)
        
        return pd.Series({
            'traffic': traffic_sum,
            'keyword': keywords,
            'url': urls,
            'occurrences': occurrences
        }, name=group.name)

    segment_analysis = analysis_df.groupby('segment').apply(agg_keywords_and_urls).reset_index()
    segment_analysis = segment_analysis.sort_values('traffic', ascending=False)

    # Create partial segment analysis
    partial_segment_analysis = segment_analysis[
        (segment_analysis['occurrences'] > 5) & 
        (segment_analysis['occurrences'] <= 50)
    ]

    # Format traffic values with commas
    for df in [top_pages_sem, segment_analysis, partial_segment_analysis]:
        if 'traffic' in df.columns:
            # Create an explicit copy to avoid the SettingWithCopyWarning
            df_copy = df.copy()
            df_copy.loc[:, 'traffic'] = df_copy['traffic'].apply(lambda x: f"{x:,}" if isinstance(x, (int, float)) else x)
            df = df_copy

    return top_pages_sem, segment_analysis, partial_segment_analysis

def get_download_link(df, filename):
    """Generate a download link for a dataframe"""
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    href = f'data:file/csv;base64,{b64}'
    return href

def create_zip_download(dfs, filenames):
    """Create a ZIP file containing multiple CSV files"""
    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:
        for df, filename in zip(dfs, filenames):
            csv_buffer = io.StringIO()
            df.to_csv(csv_buffer, index=False)
            zf.writestr(filename, csv_buffer.getvalue())
    
    zip_buffer.seek(0)
    b64 = base64.b64encode(zip_buffer.getvalue()).decode()
    href = f'data:application/zip;base64,{b64}'
    return href

def main():
    # Title and subtitle
    st.markdown("# SEMrush Organic Position Combo Tool")
    st.markdown("created by [Jimmy Lange](https://jamesrobertlange.com)", unsafe_allow_html=True)
    
    # Initialize these variables to None at the start
    top_pages_sem = None
    full_segment_analysis = None
    partial_segment_analysis = None
    
    # Sidebar controls
    with st.sidebar:
        st.header("Settings")
        
        # File upload with size limit warning
        st.markdown("""
            ### Upload CSV Files
            
            ⚠️ **File Size Limits**
            - Maximum file size: 200MB per file
            - Larger files may cause performance issues
        """)
        
        uploaded_files = st.file_uploader(
            "Upload SEMrush CSV files",
            type=['csv'],
            accept_multiple_files=True,
            label_visibility="hidden"
        )
        
        # Check file sizes
        if uploaded_files:
            for file in uploaded_files:
                file_size = file.size / (1024 * 1024)  # Convert to MB
                if file_size > 200:
                    st.error(f"⚠️ {file.name} is {file_size:.1f}MB. Files over 200MB may fail to process on Streamlit Community Cloud.")
        
        # Add segment toggle in sidebar
        st.sidebar.markdown("### Output Options")
        include_segments = st.sidebar.checkbox(
            "Include Segments in Combined Output",
            value=False
        )
        
        max_position = st.number_input(
            "Maximum Position (1-100)",
            min_value=1,
            max_value=100,
            value=11
        )
        
        branded_input = st.text_input(
            "Branded Terms",
            placeholder="e.g., client name, client, client"
        ).strip()
        branded_terms = branded_input.lower().split(',') if branded_input else []

    if uploaded_files:
        with st.spinner("Processing files..."):
            try:
                # Process the files and properly capture the return values
                top_pages_sem, full_segment_analysis, partial_segment_analysis = process_csv_files(
                    uploaded_files,
                    max_position,
                    branded_terms,
                    include_segments
                )
                
                st.success(f"Total rows processed: {len(top_pages_sem)}")

                # Display results in tabs
                tab1, tab2, tab3 = st.tabs([
                    "Combined CSV SEMrush Output",
                    "Full Segment Analysis",
                    "Partial Segment Analysis"
                ])

                with tab1:
                    st.header("Combined CSV SEMrush Output Sample Data")
                    st.dataframe(top_pages_sem.head())

                with tab2:
                    st.header("Full Segment Analysis Sample Data")
                    st.markdown("""
                    This analysis includes ALL segments (URL paths) and their metrics, showing:
                    - Total traffic for each segment
                    - Number of times the segment appears
                    - Top 3 keywords and URLs for each segment
                    Sorted by total traffic.
                    """)
                    st.dataframe(full_segment_analysis.head())

                with tab3:
                    st.header("Partial Segment Analysis Sample Data")
                    st.markdown("""
                    This analysis includes only segments that appear 5-50 times in the data. This helps identify:
                    - Mid-volume content areas
                    - Sections that aren't main landing pages but still drive traffic
                    - Potential optimization opportunities
                    Excludes very high-volume (>50 occurrences) and very low-volume (<5 occurrences) segments.
                    """)
                    if partial_segment_analysis.empty:
                        st.info("No segments match the partial analysis criteria")
                    else:
                        st.dataframe(partial_segment_analysis.head())

                # Download section
                st.markdown("## Download Results")

                # Main download button
                col_main = st.container()
                with col_main:
                    st.markdown('<div class="main-download-button">', unsafe_allow_html=True)
                    st.download_button(
                        label="📥 Download Combined SEMrush CSV",
                        data=top_pages_sem.to_csv(index=False),
                        file_name="combined_semrush_output.csv",
                        mime="text/csv",
                        key="main_download"  # Added unique key
                    )
                    st.markdown('</div>', unsafe_allow_html=True)

                # Divider with consistent styling
                st.markdown('<div class="divider">Additional Downloads</div>', unsafe_allow_html=True)

                # Secondary downloads
                st.markdown('<div class="secondary-downloads">', unsafe_allow_html=True)
                col1, col2 = st.columns(2)
                
                with col1:
                    st.download_button(
                        label="📊 Full Segment Analysis",
                        data=full_segment_analysis.to_csv(index=False),
                        file_name="full_segment_analysis.csv",
                        mime="text/csv",
                        key="full_segment_download"  # Added unique key
                    )
                
                with col2:
                    st.download_button(
                        label="📈 Partial Segment Analysis",
                        data=partial_segment_analysis.to_csv(index=False),
                        file_name="partial_segment_analysis.csv",
                        mime="text/csv",
                        key="partial_segment_download"  # Added unique key
                    )
                st.markdown('</div>', unsafe_allow_html=True)

                # ZIP download
                zip_href = create_zip_download(
                    [top_pages_sem, full_segment_analysis, partial_segment_analysis],
                    ["combined_semrush_output.csv", "full_segment_analysis.csv", "partial_segment_analysis.csv"]
                )
                st.markdown(f"""
                    <div class='zip-container'>
                        <a href="{zip_href}" download="analysis_results.zip" aria-label="Download all results as ZIP file">
                            📦 Download All Results (ZIP)
                        </a>
                    </div>
                """, unsafe_allow_html=True)

            except Exception as e:
                st.error(f"An error occurred: {str(e)}")
    else:
        st.info("Please upload CSV files to begin processing")

if __name__ == "__main__":
    main()

================
File: README.md
================
# SEMrush CSV Combiner and Processor - Streamlit App

A Streamlit web application for processing and analyzing SEO data from multiple SEMrush CSV exports. This app combines data sources, performs segment analysis, and provides downloadable reports.

## Features

- Upload and process multiple SEMrush CSV files simultaneously
- Configurable maximum position filter (1-100)
- Custom branded terms filtering
- Optional segment inclusion in main output
- Three types of analysis outputs:
  - Combined SEMrush Output (main dataset)
  - Full Segment Analysis
  - Partial Segment Analysis

## Outputs Explained

### 1. Combined SEMrush Output
- Primary output combining all input CSV files
- Includes basic SEMrush data: keyword, position, search volume, etc.
- Optional segment column (can be toggled in sidebar)
- Branded term identification (if terms provided)
- Cleaned and standardized column names

### 2. Full Segment Analysis
- Analyzes ALL URL segments (paths) in your data
- Shows total traffic per segment
- Counts segment occurrences
- Lists top 3 keywords and URLs per segment
- Useful for understanding overall site structure performance

### 3. Partial Segment Analysis
- Focuses on "mid-volume" segments (5-50 occurrences)
- Excludes very high-volume segments (>50 occurrences)
- Excludes low-volume segments (<5 occurrences)
- Helps identify optimization opportunities in moderately-used sections
- Perfect for finding "hidden gem" content areas

## File Requirements

Input CSV files should include these columns:
- Keyword
- Position
- Search Volume
- Keyword Intents
- URL
- Traffic
- Timestamp

## Installation and Setup

1. Clone this repository:
```bash
git clone https://github.com/yourusername/semrush-processor-streamlit
cd semrush-processor-streamlit
```

2. Create a virtual environment (optional but recommended):
```bash
python -m venv venv
source venv/bin/activate  # On Windows, use: venv\Scripts\activate
```

3. Install required packages:
```bash
pip install -r requirements.txt
```

4. Run the application:
```bash
streamlit run app.py
```

## Usage Instructions

1. **Upload Files**
   - Use the sidebar file uploader to select one or more SEMrush CSV exports
   - Files must be under 200MB each for Streamlit Cloud deployment

2. **Configure Settings** (in sidebar)
   - Set maximum position filter (1-100)
   - Enter branded terms if needed (comma-separated)
   - Toggle segment inclusion in main output
   - Watch for file size warnings

3. **View Results**
   - Combined SEMrush Output: Main combined dataset
   - Full Segment Analysis: Complete URL path analysis
   - Partial Segment Analysis: Mid-volume segment opportunities

4. **Download Options**
   - Large centered button for main Combined SEMrush CSV
   - Secondary buttons for segment analyses
   - All-in-one ZIP download with all reports

## Size Limits

- Streamlit Community Cloud: 200MB per file
- Local deployment: Limited only by system memory
- For larger files:
  - Split files before uploading
  - Run locally
  - Use Streamlit Enterprise (if available)

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

================
File: requirements.txt
================
streamlit>=1.24.0
pandas>=1.5.0
numpy>=1.24.0
python-dateutil>=2.8.2
pytz>=2023.3
plotly>=5.13.0  # Optional: for enhanced visualizations
openpyxl>=3.1.0  # For Excel file support if needed
